{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import gspread\n",
    "import os\n",
    "from typing import Tuple, Dict, List, Any\n",
    "from datetime import datetime\n",
    "\n",
    "SHEET_NAMES = {\n",
    "    'STOCK_INFLOW': 'stock_inflow',\n",
    "    'RELEASE': 'release',\n",
    "    'STOCK_INFLOW_CLEAN': 'stock_inflow_clean',\n",
    "    'OPENING_STOCK': 'opening_stock',\n",
    "    'RELEASE_CLEAN': 'release_clean',\n",
    "    'SUMMARY': 'summary'\n",
    "}\n",
    "\n",
    "PRODUCT_TYPES = {\n",
    "    'CHICKEN': 'chicken',\n",
    "    'GIZZARD': 'gizzard'\n",
    "}\n",
    "\n",
    "DATE_FORMATS = ['%d %b %Y', '%d/%m/%y']\n",
    "GOOGLE_SHEETS_SCOPE = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "class DataProcessingError(Exception):\n",
    "    \"\"\"Custom exception for data processing errors\"\"\"\n",
    "    pass\n",
    "\n",
    "def get_credentials(credentials_file: str) -> service_account.Credentials:\n",
    "    \"\"\"Create and return credentials for Google Sheets access\"\"\"\n",
    "    try:\n",
    "        return service_account.Credentials.from_service_account_file(\n",
    "            credentials_file,\n",
    "            scopes=GOOGLE_SHEETS_SCOPE\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to create credentials: {str(e)}\")\n",
    "\n",
    "def connect_to_sheets(credentials: service_account.Credentials) -> gspread.Spreadsheet:\n",
    "    try:\n",
    "        gc = gspread.authorize(credentials)\n",
    "        spreadsheet_url = os.getenv('SPREADSHEET_URL')\n",
    "        if not spreadsheet_url:\n",
    "            raise DataProcessingError(\"SPREADSHEET_URL environment variable not set\")\n",
    "        return gc.open_by_url(spreadsheet_url)\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to connect to Google Sheets: {str(e)}\")\n",
    "\n",
    "def read_worksheet_to_df(spreadsheet: gspread.Spreadsheet, worksheet_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "        all_values = worksheet.get_all_values()\n",
    "        if not all_values:\n",
    "            raise DataProcessingError(f\"No data found in worksheet {worksheet_name}\")\n",
    "        \n",
    "        headers = all_values[0]\n",
    "        data = all_values[1:]\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        if 'date' in df.columns:\n",
    "            print(f\"\\nUnique date values in {worksheet_name}:\")\n",
    "            # print(df['date'].unique())\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to read worksheet {worksheet_name}: {str(e)}\")\n",
    "\n",
    "def standardize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        print(\"\\nStandardizing dataframe...\")\n",
    "        # print(\"Original columns:\", df.columns.tolist())\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Standardize column names\n",
    "        df_clean.columns = (df_clean.columns.str.lower()\n",
    "                          .str.strip()\n",
    "                          .str.replace(' ', '_')\n",
    "                          .str.replace('-', '_'))\n",
    "        \n",
    "        # print(\"Standardized columns:\", df_clean.columns.tolist())\n",
    "        \n",
    "        # Handle the weight_in_kg to weight rename\n",
    "        if 'weight_in_kg' in df_clean.columns:\n",
    "            df_clean = df_clean.rename(columns={'weight_in_kg': 'weight'})\n",
    "        \n",
    "        for column in df_clean.columns:\n",
    "            df_clean[column] = df_clean[column].astype(str).str.strip().str.lower()\n",
    "            try:\n",
    "                numeric_values = pd.to_numeric(df_clean[column].str.replace(',', ''))\n",
    "                df_clean[column] = numeric_values\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        return df_clean\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to standardize dataframe: {str(e)}\")\n",
    "\n",
    "def standardize_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nStandardizing dates...\")\n",
    "        df = df.copy()\n",
    "        # print(\"Original unique date values:\", df['date'].unique())\n",
    "        \n",
    "        date_parsed = False\n",
    "        for format in DATE_FORMATS:\n",
    "            try:\n",
    "                print(f\"Trying date format: {format}\")\n",
    "                df['date'] = pd.to_datetime(df['date'], format=format)\n",
    "                date_parsed = True\n",
    "                print(\"Successfully parsed dates using format:\", format)\n",
    "                break\n",
    "            except ValueError as e:\n",
    "                print(f\"Failed to parse with format {format}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not date_parsed:\n",
    "            print(\"Falling back to mixed format parsing\")\n",
    "            df['date'] = pd.to_datetime(df['date'], format='mixed', dayfirst=True)\n",
    "        \n",
    "        if df['date'].isna().any():\n",
    "            problematic_dates = df[df['date'].isna()]['date'].unique()\n",
    "            print(\"Warning: Failed to parse these dates:\", problematic_dates)\n",
    "            raise DataProcessingError(f\"Failed to parse dates: {problematic_dates}\")\n",
    "        \n",
    "        df['month'] = df['date'].dt.strftime('%b').str.lower()\n",
    "        df['year_month'] = df['date'].dt.strftime('%Y-%b')\n",
    "        \n",
    "        # print(\"Processed unique months:\", df['month'].unique())\n",
    "        # print(\"Processed unique year_months:\", df['year_month'].unique())\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to standardize dates: {str(e)}\")\n",
    "\n",
    "def remove_opening_stock(df: pd.DataFrame, column_name: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    try:\n",
    "        print(f\"\\nRemoving opening stock using column: {column_name}\")\n",
    "        opening_stock_mask = df[column_name].str.contains('opening stock', case=False, na=False)\n",
    "        opening_stock_df = df[opening_stock_mask].copy()\n",
    "        main_df = df[~opening_stock_mask].copy()\n",
    "        \n",
    "        # print(f\"Found {len(opening_stock_df)} opening stock entries\")\n",
    "        print(f\"Remaining entries: {len(main_df)}\")\n",
    "        \n",
    "        return main_df, opening_stock_df\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to separate opening stock: {str(e)}\")\n",
    "\n",
    "def create_summary_df(stock_inflow_df: pd.DataFrame, release_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        print(\"\\nCreating summary dataframe...\")\n",
    "        \n",
    "        # print(\"\\nStock Inflow Unique Year-Months:\", stock_inflow_df['year_month'].unique())\n",
    "        # print(\"Release Unique Year-Months:\", release_df['year_month'].unique())\n",
    "        \n",
    "        all_year_months = sorted(list(set(stock_inflow_df['year_month'].unique()) | \n",
    "                                    set(release_df['year_month'].unique())))\n",
    "        \n",
    "        summary_df = pd.DataFrame({'year_month': all_year_months})\n",
    "        summary_df['month'] = summary_df['year_month'].str.split('-').str[1].str.lower()\n",
    "        summary_df = summary_df[['month', 'year_month']]\n",
    "                \n",
    "        product_summaries = {\n",
    "            'chicken_inflow': stock_inflow_df[\n",
    "                stock_inflow_df['product_type'] == PRODUCT_TYPES['CHICKEN']\n",
    "            ].groupby('year_month').agg({\n",
    "                'quantity': 'sum',\n",
    "                'weight': 'sum'\n",
    "            }),\n",
    "            'chicken_release': release_df[\n",
    "                release_df['product'] == PRODUCT_TYPES['CHICKEN']\n",
    "            ].groupby('year_month').agg({\n",
    "                'quantity': 'sum',\n",
    "                'weight': 'sum'\n",
    "            }),\n",
    "            'gizzard_inflow': stock_inflow_df[\n",
    "                stock_inflow_df['product_type'] == PRODUCT_TYPES['GIZZARD']\n",
    "            ].groupby('year_month').agg({\n",
    "                'weight': 'sum'\n",
    "            }),\n",
    "            'gizzard_release': release_df[\n",
    "                release_df['product'] == PRODUCT_TYPES['GIZZARD']\n",
    "            ].groupby('year_month').agg({\n",
    "                'weight': 'sum'\n",
    "            })\n",
    "        }\n",
    "        \n",
    "        summary_columns = {\n",
    "            'total_chicken_inflow_quantity': ('chicken_inflow', 'quantity'),\n",
    "            'total_chicken_inflow_weight': ('chicken_inflow', 'weight'),\n",
    "            'total_chicken_release_quantity': ('chicken_release', 'quantity'),\n",
    "            'total_chicken_release_weight': ('chicken_release', 'weight'),\n",
    "            'total_gizzard_inflow_weight': ('gizzard_inflow', 'weight'),\n",
    "            'total_gizzard_release_weight': ('gizzard_release', 'weight')\n",
    "        }\n",
    "        \n",
    "        for col_name, (summary_key, metric) in summary_columns.items():\n",
    "            if metric in product_summaries[summary_key].columns:\n",
    "                summary_df[col_name] = summary_df['year_month'].map(\n",
    "                    product_summaries[summary_key][metric]).fillna(0)\n",
    "            else:\n",
    "                summary_df[col_name] = 0\n",
    "\n",
    "        # Sort by year_month and calculate running balances\n",
    "        summary_df['sort_date'] = pd.to_datetime(summary_df['year_month'], format='%Y-%b')\n",
    "        summary_df = summary_df.sort_values('sort_date')\n",
    "\n",
    "        # Calculate and carry forward balances\n",
    "        previous_balances = {\n",
    "            'chicken_quantity': 0,\n",
    "            'chicken_weight': 0,\n",
    "            'gizzard_weight': 0\n",
    "        }\n",
    "\n",
    "        for index in summary_df.index:\n",
    "            # Add previous balances to current inflow\n",
    "            summary_df.loc[index, 'total_chicken_inflow_quantity'] += previous_balances['chicken_quantity']\n",
    "            summary_df.loc[index, 'total_chicken_inflow_weight'] += previous_balances['chicken_weight']\n",
    "            summary_df.loc[index, 'total_gizzard_inflow_weight'] += previous_balances['gizzard_weight']\n",
    "\n",
    "            # Calculate new balances\n",
    "            previous_balances['chicken_quantity'] = max(0, \n",
    "                summary_df.loc[index, 'total_chicken_inflow_quantity'] - \n",
    "                summary_df.loc[index, 'total_chicken_release_quantity'])\n",
    "            \n",
    "            previous_balances['chicken_weight'] = max(0,\n",
    "                summary_df.loc[index, 'total_chicken_inflow_weight'] - \n",
    "                summary_df.loc[index, 'total_chicken_release_weight'])\n",
    "            \n",
    "            previous_balances['gizzard_weight'] = max(0,\n",
    "                summary_df.loc[index, 'total_gizzard_inflow_weight'] - \n",
    "                summary_df.loc[index, 'total_gizzard_release_weight'])\n",
    "\n",
    "        # Sort and format for output\n",
    "        summary_df = summary_df.sort_values('sort_date', ascending=False)\n",
    "        summary_df['year_month'] = summary_df['sort_date'].dt.strftime('%Y-%m')\n",
    "        summary_df = summary_df.drop('sort_date', axis=1)\n",
    "        \n",
    "        # print(\"\\nFinal summary dataframe structure:\")\n",
    "        # print(\"\\nSummary columns:\", summary_df.columns.tolist())\n",
    "        \n",
    "        return summary_df\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to create summary: {str(e)}\")\n",
    "\n",
    "def prepare_df_for_upload(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"\\nPreparing dataframe for upload...\")\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    date_columns = df_copy.select_dtypes(include=['datetime64']).columns\n",
    "    for col in date_columns:\n",
    "        df_copy[col] = df_copy[col].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    for col in df_copy.columns:\n",
    "        df_copy[col] = df_copy[col].fillna('')\n",
    "        df_copy[col] = df_copy[col].astype(str)\n",
    "        df_copy[col] = df_copy[col].replace('nan', '')\n",
    "    \n",
    "    # print(f\"Prepared {len(df_copy)} rows for upload\")\n",
    "    return df_copy\n",
    "\n",
    "def upload_df_to_gsheet(df: pd.DataFrame, \n",
    "                       spreadsheet_id: str, \n",
    "                       sheet_name: str, \n",
    "                       service: Any) -> bool:\n",
    "    try:\n",
    "        print(f\"\\nUploading data to sheet: {sheet_name}\")\n",
    "        df_to_upload = prepare_df_for_upload(df)\n",
    "        \n",
    "        values = [df_to_upload.columns.tolist()]\n",
    "        values.extend([[str(cell) if cell is not None and cell == cell else '' \n",
    "                       for cell in row] for row in df_to_upload.values.tolist()])\n",
    "        \n",
    "        service.spreadsheets().values().clear(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            range=f'{sheet_name}!A1:ZZ'\n",
    "        ).execute()\n",
    "        \n",
    "        result = service.spreadsheets().values().update(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            range=f'{sheet_name}!A1',\n",
    "            valueInputOption='RAW',\n",
    "            body={'values': values}\n",
    "        ).execute()\n",
    "        \n",
    "        print(f\"Updated {result.get('updatedCells')} cells in {sheet_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload to {sheet_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_sheets_data(stock_inflow_df: pd.DataFrame, \n",
    "                       release_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, \n",
    "                                                        pd.DataFrame, pd.DataFrame]:\n",
    "    try:\n",
    "        print(\"\\nProcessing sheets data...\")\n",
    "        \n",
    "        stock_inflow_df = standardize_dataframe(stock_inflow_df)\n",
    "        release_df = standardize_dataframe(release_df)\n",
    "        \n",
    "        stock_inflow_df = standardize_dates(stock_inflow_df)\n",
    "        release_df = standardize_dates(release_df)\n",
    "        \n",
    "        stock_inflow_main_df, opening_stock_df = remove_opening_stock(\n",
    "            stock_inflow_df, 'purchasing_officer')\n",
    "        release_df, _ = remove_opening_stock(release_df, 'name_of_collector')\n",
    "        \n",
    "        release_df.loc[\n",
    "            release_df['product'].str.contains(PRODUCT_TYPES['GIZZARD'], \n",
    "                                             case=False, na=False), \n",
    "            'quantity'\n",
    "        ] = 0\n",
    "        \n",
    "        summary_df = create_summary_df(stock_inflow_main_df, release_df)\n",
    "        \n",
    "        return stock_inflow_main_df, opening_stock_df, release_df, summary_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to process sheets data: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    CREDENTIALS_FILE = 'credentials.json'\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nStarting data processing...\")\n",
    "        \n",
    "        spreadsheet_id = os.getenv('SPREADSHEET_ID')\n",
    "        if not spreadsheet_id:\n",
    "            raise DataProcessingError(\"SPREADSHEET_ID environment variable not set\")\n",
    "            \n",
    "        # Create credentials and services once\n",
    "        credentials = get_credentials(CREDENTIALS_FILE)\n",
    "        spreadsheet = connect_to_sheets(credentials)\n",
    "        sheets_service = build('sheets', 'v4', credentials=credentials)\n",
    "        \n",
    "        # Read the worksheets\n",
    "        stock_inflow_df = read_worksheet_to_df(spreadsheet, SHEET_NAMES['STOCK_INFLOW'])\n",
    "        release_df = read_worksheet_to_df(spreadsheet, SHEET_NAMES['RELEASE'])\n",
    "        \n",
    "        # Process the data\n",
    "        stock_inflow_main_df, opening_stock_df, release_df, summary_df = process_sheets_data(\n",
    "            stock_inflow_df, release_df)\n",
    "        \n",
    "        # Define upload tasks\n",
    "        upload_tasks = [\n",
    "            (stock_inflow_main_df, SHEET_NAMES['STOCK_INFLOW_CLEAN']),\n",
    "            (opening_stock_df, SHEET_NAMES['OPENING_STOCK']),\n",
    "            (release_df, SHEET_NAMES['RELEASE_CLEAN']),\n",
    "            (summary_df, SHEET_NAMES['SUMMARY'])\n",
    "        ]\n",
    "        \n",
    "        # Upload all datasets\n",
    "        success = True\n",
    "        for df, sheet_name in upload_tasks:\n",
    "            if not upload_df_to_gsheet(df, spreadsheet_id, sheet_name, sheets_service):\n",
    "                success = False\n",
    "                print(f\"Failed to upload {sheet_name}\")\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\nData processing and upload completed successfully!\")\n",
    "        else:\n",
    "            raise DataProcessingError(\"Failed to upload one or more datasets\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project-env]",
   "language": "python",
   "name": "conda-env-project-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
