{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import gspread\n",
    "import os\n",
    "from typing import Tuple, Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "SHEET_NAMES = {\n",
    "    'STOCK_INFLOW': 'stock_inflow',\n",
    "    'RELEASE': 'release',\n",
    "    'STOCK_INFLOW_CLEAN': 'stock_inflow_clean',\n",
    "    'OPENING_STOCK': 'opening_stock',\n",
    "    'RELEASE_CLEAN': 'release_clean',\n",
    "    'SUMMARY': 'summary'\n",
    "}\n",
    "\n",
    "PRODUCT_TYPES = {\n",
    "    'CHICKEN': 'chicken',\n",
    "    'GIZZARD': 'gizzard'\n",
    "}\n",
    "\n",
    "class DataProcessingError(Exception):\n",
    "    \"\"\"Custom exception for data processing errors\"\"\"\n",
    "    pass\n",
    "\n",
    "def connect_to_sheets(credentials_file: str) -> gspread.Spreadsheet:\n",
    "    \"\"\"Connect to Google Sheets using provided credentials\"\"\"\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            credentials_file,\n",
    "            scopes=['https://www.googleapis.com/auth/spreadsheets']\n",
    "        )\n",
    "        \n",
    "        gc = gspread.authorize(credentials)\n",
    "        spreadsheet_url = os.getenv('SPREADSHEET_URL')\n",
    "        if not spreadsheet_url:\n",
    "            raise DataProcessingError(\"SPREADSHEET_URL environment variable not set\")\n",
    "        return gc.open_by_url(spreadsheet_url)\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to connect to Google Sheets: {str(e)}\")\n",
    "\n",
    "def read_worksheet_to_df(spreadsheet: gspread.Spreadsheet, worksheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Read worksheet data into pandas DataFrame\"\"\"\n",
    "    try:\n",
    "        worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "        all_values = worksheet.get_all_values()\n",
    "        if not all_values:\n",
    "            raise DataProcessingError(f\"No data found in worksheet {worksheet_name}\")\n",
    "        headers = all_values[0]\n",
    "        data = all_values[1:]\n",
    "        return pd.DataFrame(data, columns=headers)\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to read worksheet {worksheet_name}: {str(e)}\")\n",
    "\n",
    "def standardize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize column names and data types\"\"\"\n",
    "    try:\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Standardize column names\n",
    "        df_clean.columns = (df_clean.columns.str.lower()\n",
    "                          .str.strip()\n",
    "                          .str.replace(' ', '_')\n",
    "                          .str.replace('-', '_'))\n",
    "        \n",
    "        # Standardize data\n",
    "        for column in df_clean.columns:\n",
    "            df_clean[column] = df_clean[column].astype(str).str.strip().str.lower()\n",
    "            \n",
    "            # Try converting to numeric if possible\n",
    "            try:\n",
    "                numeric_values = pd.to_numeric(df_clean[column].str.replace(',', ''))\n",
    "                df_clean[column] = numeric_values\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        return df_clean\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to standardize dataframe: {str(e)}\")\n",
    "\n",
    "def standardize_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize date columns and add derived date columns\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    try:\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        date_formats = ['%d %b %Y', '%d/%m/%y']\n",
    "        for format in date_formats:\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(df['date'], format=format)\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "        else:\n",
    "            df['date'] = pd.to_datetime(df['date'], format='mixed', dayfirst=True)\n",
    "        \n",
    "        # Add derived date columns while keeping date as datetime\n",
    "        df['month'] = df['date'].dt.strftime('%b').str.lower()\n",
    "        df['year_month'] = df['date'].dt.strftime('%Y-%b')\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to standardize dates: {str(e)}\")\n",
    "\n",
    "def remove_opening_stock(df: pd.DataFrame, column_name: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Separate opening stock records from main data\"\"\"\n",
    "    try:\n",
    "        opening_stock_mask = df[column_name].str.contains('opening stock', case=False, na=False)\n",
    "        opening_stock_df = df[opening_stock_mask].copy()\n",
    "        main_df = df[~opening_stock_mask].copy()\n",
    "        return main_df, opening_stock_df\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to separate opening stock: {str(e)}\")\n",
    "\n",
    "def create_summary_df(stock_inflow_df: pd.DataFrame, release_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create summary DataFrame from stock inflow and release data\"\"\"\n",
    "    try:\n",
    "        # Get unique months and year_months\n",
    "        all_months = sorted(list(set(stock_inflow_df['month'].unique()) | \n",
    "                               set(release_df['month'].unique())))\n",
    "        all_year_months = sorted(list(set(stock_inflow_df['year_month'].unique()) | \n",
    "                                    set(release_df['year_month'].unique())))\n",
    "        \n",
    "        # Create base summary DataFrame\n",
    "        summary_df = pd.DataFrame({\n",
    "            'month': all_months,\n",
    "            'year_month': all_year_months\n",
    "        })\n",
    "        \n",
    "        # Calculate product summaries\n",
    "        product_summaries = {\n",
    "            'chicken_inflow': stock_inflow_df[\n",
    "                stock_inflow_df['product_type'] == PRODUCT_TYPES['CHICKEN']\n",
    "            ].groupby('month').agg({\n",
    "                'quantity': 'sum',\n",
    "                'weight': 'sum'\n",
    "            }),\n",
    "            'chicken_release': release_df[\n",
    "                release_df['product'] == PRODUCT_TYPES['CHICKEN']\n",
    "            ].groupby('month').agg({\n",
    "                'quantity': 'sum',\n",
    "                'weight_in_kg': 'sum'\n",
    "            }),\n",
    "            'gizzard_inflow': stock_inflow_df[\n",
    "                stock_inflow_df['product_type'] == PRODUCT_TYPES['GIZZARD']\n",
    "            ].groupby('month').agg({\n",
    "                'weight': 'sum'\n",
    "            }),\n",
    "            'gizzard_release': release_df[\n",
    "                release_df['product'] == PRODUCT_TYPES['GIZZARD']\n",
    "            ].groupby('month').agg({\n",
    "                'weight_in_kg': 'sum'\n",
    "            })\n",
    "        }\n",
    "        \n",
    "        # Map values to summary DataFrame\n",
    "        summary_columns = {\n",
    "            'total_chicken_inflow_quantity': ('chicken_inflow', 'quantity'),\n",
    "            'total_chicken_inflow_weight': ('chicken_inflow', 'weight'),\n",
    "            'total_chicken_release_quantity': ('chicken_release', 'quantity'),\n",
    "            'total_chicken_release_weight': ('chicken_release', 'weight_in_kg'),\n",
    "            'total_gizzard_inflow_weight': ('gizzard_inflow', 'weight'),\n",
    "            'total_gizzard_release_weight': ('gizzard_release', 'weight_in_kg')\n",
    "        }\n",
    "        \n",
    "        for col_name, (summary_key, metric) in summary_columns.items():\n",
    "            if metric in product_summaries[summary_key].columns:\n",
    "                summary_df[col_name] = summary_df['month'].map(\n",
    "                    product_summaries[summary_key][metric]).fillna(0)\n",
    "            else:\n",
    "                summary_df[col_name] = 0\n",
    "        \n",
    "        return summary_df\n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to create summary: {str(e)}\")\n",
    "\n",
    "def prepare_df_for_upload(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Prepare DataFrame for upload by converting all data types to proper format\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Convert datetime columns to string format\n",
    "    date_columns = df_copy.select_dtypes(include=['datetime64']).columns\n",
    "    for col in date_columns:\n",
    "        df_copy[col] = df_copy[col].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Handle NaN values and convert all columns to string\n",
    "    for col in df_copy.columns:\n",
    "        # Convert NaN/None to empty string\n",
    "        df_copy[col] = df_copy[col].fillna('')\n",
    "        # Convert all values to string\n",
    "        df_copy[col] = df_copy[col].astype(str)\n",
    "        # Replace 'nan' strings with empty string\n",
    "        df_copy[col] = df_copy[col].replace('nan', '')\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def upload_df_to_gsheet(df: pd.DataFrame, spreadsheet_id: str, \n",
    "                       sheet_name: str, credentials_file: str) -> bool:\n",
    "    \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "    try:\n",
    "        # Prepare data for upload\n",
    "        df_to_upload = prepare_df_for_upload(df)\n",
    "        \n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            credentials_file,\n",
    "            scopes=['https://www.googleapis.com/auth/spreadsheets']\n",
    "        )\n",
    "        \n",
    "        service = build('sheets', 'v4', credentials=credentials)\n",
    "        \n",
    "        # Convert DataFrame to list of lists for upload\n",
    "        values = [df_to_upload.columns.tolist()]\n",
    "        # Convert all values to strings and replace any remaining NaN\n",
    "        values.extend([[str(cell) if cell is not None and cell == cell else '' \n",
    "                       for cell in row] for row in df_to_upload.values.tolist()])\n",
    "        \n",
    "        # Clear existing content\n",
    "        service.spreadsheets().values().clear(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            range=f'{sheet_name}!A1:ZZ'\n",
    "        ).execute()\n",
    "        \n",
    "        # Upload new content\n",
    "        result = service.spreadsheets().values().update(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            range=f'{sheet_name}!A1',\n",
    "            valueInputOption='RAW',\n",
    "            body={'values': values}\n",
    "        ).execute()\n",
    "        \n",
    "        print(f\"Updated {result.get('updatedCells')} cells in {sheet_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload to {sheet_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def process_sheets_data(stock_inflow_df: pd.DataFrame, \n",
    "                       release_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, \n",
    "                                                        pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Process all sheet data and create necessary outputs\"\"\"\n",
    "    try:\n",
    "        # Initial standardization\n",
    "        stock_inflow_df = standardize_dataframe(stock_inflow_df)\n",
    "        release_df = standardize_dataframe(release_df)\n",
    "        \n",
    "        # Standardize dates\n",
    "        stock_inflow_df = standardize_dates(stock_inflow_df)\n",
    "        release_df = standardize_dates(release_df)\n",
    "        \n",
    "        # Remove opening stock records and get clean versions\n",
    "        stock_inflow_main_df, opening_stock_df = remove_opening_stock(\n",
    "            stock_inflow_df, 'purchasing_officer')\n",
    "        release_df, _ = remove_opening_stock(release_df, 'name_of_collector')\n",
    "        \n",
    "        # Set quantity to 0 for gizzard products in release\n",
    "        release_df.loc[\n",
    "            release_df['product'].str.contains(PRODUCT_TYPES['GIZZARD'], \n",
    "                                             case=False, na=False), \n",
    "            'quantity'\n",
    "        ] = 0\n",
    "        \n",
    "        # Create summary using the cleaned dataframes\n",
    "        summary_df = create_summary_df(stock_inflow_main_df, release_df)\n",
    "        \n",
    "        return stock_inflow_main_df, opening_stock_df, release_df, summary_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise DataProcessingError(f\"Failed to process sheets data: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    CREDENTIALS_FILE = 'credentials.json'\n",
    "    \n",
    "    try:\n",
    "        # Validate environment variables\n",
    "        spreadsheet_id = os.getenv('SPREADSHEET_ID')\n",
    "        if not spreadsheet_id:\n",
    "            raise DataProcessingError(\"SPREADSHEET_ID environment variable not set\")\n",
    "        \n",
    "        # Connect and read data\n",
    "        spreadsheet = connect_to_sheets(CREDENTIALS_FILE)\n",
    "        stock_inflow_df = read_worksheet_to_df(spreadsheet, SHEET_NAMES['STOCK_INFLOW'])\n",
    "        release_df = read_worksheet_to_df(spreadsheet, SHEET_NAMES['RELEASE'])\n",
    "        \n",
    "        # Process data\n",
    "        stock_inflow_main_df, opening_stock_df, release_df, summary_df = process_sheets_data(\n",
    "            stock_inflow_df, release_df)\n",
    "        \n",
    "        # Upload all sheets\n",
    "        upload_tasks = [\n",
    "            (stock_inflow_main_df, SHEET_NAMES['STOCK_INFLOW_CLEAN']),\n",
    "            (opening_stock_df, SHEET_NAMES['OPENING_STOCK']),\n",
    "            (release_df, SHEET_NAMES['RELEASE_CLEAN']),\n",
    "            (summary_df, SHEET_NAMES['SUMMARY'])\n",
    "        ]\n",
    "        \n",
    "        success = True\n",
    "        for df, sheet_name in upload_tasks:\n",
    "            if not upload_df_to_gsheet(df, spreadsheet_id, sheet_name, CREDENTIALS_FILE):\n",
    "                success = False\n",
    "                print(f\"Failed to upload {sheet_name}\")\n",
    "        \n",
    "        if success:\n",
    "            print(\"Data processing and upload completed successfully!\")\n",
    "        else:\n",
    "            raise DataProcessingError(\"Failed to upload one or more datasets\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
